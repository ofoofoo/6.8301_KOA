# SKANs: Scaling Kolmogorov-Arnold Networks


## Introduction
This repository contains the implementation and supporting materials for scaling Kolmogorov Arnold Networks (SKANs). KANs are designed to address some of the limitations of Multi-Layer Perceptrons (MLPs) by incorporating principles from the Kolmogorov-Arnold representation theorem. The goal of this project is to explore the capabilities of KANs in scaling up to complex datasets and tasks, providing an efficient and theoretically grounded alternative to traditional neural network architectures.

## Motivation
The motivation behind Kolmogorov Arnold Networks (KANs) stems from the desire to improve the expressiveness and efficiency of neural networks. Traditional MLPs, while powerful, often require extensive data, resources, and layers to capture complex patterns effectively. KANs are inspired by the Kolmogorov-Arnold representation theorem, which suggests that any multivariate continuous function can be represented as a superposition of continuous functions of two variables. This theoretical backing promises a more compact and potentially more powerful network architecture capable of learning complex functions with fewer resources.

## Repository Structure
/
├── src/ # Source code for SKAN implementation
├── data/ # Dataset directories
├── models/ # Trained model weights and architecture files
├── notebooks/ # Jupyter notebooks for demonstrations and analysis
├── requirements.txt # Dependencies for running the code
└── README.md # Repository documentation
